{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence #</th>\n",
       "      <th>Word</th>\n",
       "      <th>POS</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>Thousands</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>of</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>demonstrators</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>have</td>\n",
       "      <td>VBP</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>marched</td>\n",
       "      <td>VBN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Sentence #           Word  POS Tag\n",
       "0  Sentence: 1      Thousands  NNS   O\n",
       "1          NaN             of   IN   O\n",
       "2          NaN  demonstrators  NNS   O\n",
       "3          NaN           have  VBP   O\n",
       "4          NaN        marched  VBN   O"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('../data/ner_dataset.csv', encoding='unicode_escape')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_grp = {}\n",
    "key = data['Sentence #'].iloc[0].split(': ')[-1]\n",
    "sentence_grp[key] = 1\n",
    "word_count = 1\n",
    "for i in data['Sentence #'][1:]:\n",
    "    # NaN is never equal to NaN element so it'll fail at NaM\n",
    "    if i == i: \n",
    "        sentence_grp[key] = word_count\n",
    "        key = i.split(': ')[-1]\n",
    "        word_count = 1\n",
    "    word_count += 1\n",
    "\n",
    "sentence_grp = pd.DataFrame.from_dict(sentence_grp, orient='index', columns=['#words'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape:  (1048575, 4)\n",
      "Total number of Sentences:  47958\n",
      "#words analysis:\n",
      "              #words\n",
      "count  47958.000000\n",
      "mean      22.864256\n",
      "std        7.963507\n",
      "min        2.000000\n",
      "25%       17.000000\n",
      "50%       22.000000\n",
      "75%       28.000000\n",
      "max      105.000000\n"
     ]
    }
   ],
   "source": [
    "n_sentences = data['Sentence #'].nunique() - 1 # -1 for NaN\n",
    "\n",
    "print('Shape: ', data.shape)\n",
    "print('Total number of Sentences: ', n_sentences)\n",
    "print('#words analysis:\\n', sentence_grp.describe())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tags Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique #Tags:  17\n",
      "\n",
      "----Tags----\n",
      "0         O\n",
      "1     B-geo\n",
      "2     B-gpe\n",
      "3     B-per\n",
      "4     I-geo\n",
      "5     B-org\n",
      "6     I-org\n",
      "7     B-tim\n",
      "8     B-art\n",
      "9     I-art\n",
      "10    I-per\n",
      "11    I-gpe\n",
      "12    I-tim\n",
      "13    B-nat\n",
      "14    B-eve\n",
      "15    I-eve\n",
      "16    I-nat\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "n_tags = data['Tag'].nunique()\n",
    "print('Unique #Tags: ', n_tags)\n",
    "print('\\n----Tags----')\n",
    "print(pd.Series(data['Tag'].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Mappings Req. For Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dict_map(data, token_or_tag):\n",
    "    tok2idx = {}\n",
    "    idx2tok = {}\n",
    "    \n",
    "    if token_or_tag == 'token':\n",
    "        vocab = list(set(data['Word'].to_list()))\n",
    "    else:\n",
    "        vocab = list(set(data['Tag'].to_list()))\n",
    "    \n",
    "    idx2tok = {(idx+1):tok for idx, tok in enumerate(vocab)}\n",
    "    tok2idx = {tok:(idx+1) for idx, tok in enumerate(vocab)}\n",
    "    \n",
    "    return tok2idx, idx2tok\n",
    "\n",
    "token2idx, idx2token = get_dict_map(data, 'token')\n",
    "tag2idx, idx2tag = get_dict_map(data, 'tag')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence #</th>\n",
       "      <th>Word</th>\n",
       "      <th>POS</th>\n",
       "      <th>Tag</th>\n",
       "      <th>Word_idx</th>\n",
       "      <th>Tag_idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>Thousands</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "      <td>21503</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>of</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "      <td>20142</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>demonstrators</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "      <td>29955</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>have</td>\n",
       "      <td>VBP</td>\n",
       "      <td>O</td>\n",
       "      <td>3023</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>marched</td>\n",
       "      <td>VBN</td>\n",
       "      <td>O</td>\n",
       "      <td>26343</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Sentence #           Word  POS Tag  Word_idx  Tag_idx\n",
       "0  Sentence: 1      Thousands  NNS   O     21503       16\n",
       "1          NaN             of   IN   O     20142       16\n",
       "2          NaN  demonstrators  NNS   O     29955       16\n",
       "3          NaN           have  VBP   O      3023       16\n",
       "4          NaN        marched  VBN   O     26343       16"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Word_idx'] = data['Word'].map(token2idx)\n",
    "data['Tag_idx'] = data['Tag'].map(tag2idx)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform columns to extract sequential data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence #</th>\n",
       "      <th>Word</th>\n",
       "      <th>POS</th>\n",
       "      <th>Tag</th>\n",
       "      <th>Word_idx</th>\n",
       "      <th>Tag_idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>[Thousands, of, demonstrators, have, marched, ...</td>\n",
       "      <td>[NNS, IN, NNS, VBP, VBN, IN, NNP, TO, VB, DT, ...</td>\n",
       "      <td>[O, O, O, O, O, O, B-geo, O, O, O, O, O, B-geo...</td>\n",
       "      <td>[21503, 20142, 29955, 3023, 26343, 7247, 16881...</td>\n",
       "      <td>[16, 16, 16, 16, 16, 16, 8, 16, 16, 16, 16, 16...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sentence: 10</td>\n",
       "      <td>[Iranian, officials, say, they, expect, to, ge...</td>\n",
       "      <td>[JJ, NNS, VBP, PRP, VBP, TO, VB, NN, TO, JJ, J...</td>\n",
       "      <td>[B-gpe, O, O, O, O, O, O, O, O, O, O, O, O, O,...</td>\n",
       "      <td>[34699, 2777, 12027, 26942, 6283, 9453, 16100,...</td>\n",
       "      <td>[6, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sentence: 100</td>\n",
       "      <td>[Helicopter, gunships, Saturday, pounded, mili...</td>\n",
       "      <td>[NN, NNS, NNP, VBD, JJ, NNS, IN, DT, NNP, JJ, ...</td>\n",
       "      <td>[O, O, B-tim, O, O, O, O, O, B-geo, O, O, O, O...</td>\n",
       "      <td>[5914, 32317, 15375, 25297, 23958, 25121, 5223...</td>\n",
       "      <td>[16, 16, 5, 16, 16, 16, 16, 16, 8, 16, 16, 16,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sentence: 1000</td>\n",
       "      <td>[They, left, after, a, tense, hour-long, stand...</td>\n",
       "      <td>[PRP, VBD, IN, DT, NN, JJ, NN, IN, NN, NNS, .]</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "      <td>[33792, 22369, 14755, 11604, 7264, 17234, 3225...</td>\n",
       "      <td>[16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sentence: 10000</td>\n",
       "      <td>[U.N., relief, coordinator, Jan, Egeland, said...</td>\n",
       "      <td>[NNP, NN, NN, NNP, NNP, VBD, NNP, ,, NNP, ,, J...</td>\n",
       "      <td>[B-geo, O, O, B-per, I-per, O, B-tim, O, B-geo...</td>\n",
       "      <td>[1163, 30497, 6440, 29893, 7032, 5838, 11838, ...</td>\n",
       "      <td>[8, 16, 16, 13, 1, 16, 5, 16, 8, 16, 6, 16, 6,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Sentence #                                               Word  \\\n",
       "0      Sentence: 1  [Thousands, of, demonstrators, have, marched, ...   \n",
       "1     Sentence: 10  [Iranian, officials, say, they, expect, to, ge...   \n",
       "2    Sentence: 100  [Helicopter, gunships, Saturday, pounded, mili...   \n",
       "3   Sentence: 1000  [They, left, after, a, tense, hour-long, stand...   \n",
       "4  Sentence: 10000  [U.N., relief, coordinator, Jan, Egeland, said...   \n",
       "\n",
       "                                                 POS  \\\n",
       "0  [NNS, IN, NNS, VBP, VBN, IN, NNP, TO, VB, DT, ...   \n",
       "1  [JJ, NNS, VBP, PRP, VBP, TO, VB, NN, TO, JJ, J...   \n",
       "2  [NN, NNS, NNP, VBD, JJ, NNS, IN, DT, NNP, JJ, ...   \n",
       "3     [PRP, VBD, IN, DT, NN, JJ, NN, IN, NN, NNS, .]   \n",
       "4  [NNP, NN, NN, NNP, NNP, VBD, NNP, ,, NNP, ,, J...   \n",
       "\n",
       "                                                 Tag  \\\n",
       "0  [O, O, O, O, O, O, B-geo, O, O, O, O, O, B-geo...   \n",
       "1  [B-gpe, O, O, O, O, O, O, O, O, O, O, O, O, O,...   \n",
       "2  [O, O, B-tim, O, O, O, O, O, B-geo, O, O, O, O...   \n",
       "3                  [O, O, O, O, O, O, O, O, O, O, O]   \n",
       "4  [B-geo, O, O, B-per, I-per, O, B-tim, O, B-geo...   \n",
       "\n",
       "                                            Word_idx  \\\n",
       "0  [21503, 20142, 29955, 3023, 26343, 7247, 16881...   \n",
       "1  [34699, 2777, 12027, 26942, 6283, 9453, 16100,...   \n",
       "2  [5914, 32317, 15375, 25297, 23958, 25121, 5223...   \n",
       "3  [33792, 22369, 14755, 11604, 7264, 17234, 3225...   \n",
       "4  [1163, 30497, 6440, 29893, 7032, 5838, 11838, ...   \n",
       "\n",
       "                                             Tag_idx  \n",
       "0  [16, 16, 16, 16, 16, 16, 8, 16, 16, 16, 16, 16...  \n",
       "1  [6, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16...  \n",
       "2  [16, 16, 5, 16, 16, 16, 16, 16, 8, 16, 16, 16,...  \n",
       "3       [16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16]  \n",
       "4  [8, 16, 16, 13, 1, 16, 5, 16, 8, 16, 6, 16, 6,...  "
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fill na\n",
    "data_fillna = data.fillna(method='ffill', axis=0)\n",
    "\n",
    "#Groupby and collect columns\n",
    "data_group = data_fillna.groupby(\n",
    "['Sentence #'],as_index=False).agg(lambda x: list(x))\n",
    "\n",
    "# Visualise data\n",
    "data_group.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Split the dataset into train, test after padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pad_train_test_val(data_group, data):\n",
    "    \n",
    "    #get max token and tag length\n",
    "    n_token = len(set(data['Word']))\n",
    "    n_tag = len(set(data['Tag']))\n",
    "\n",
    "    #Pad tokens (X var)\n",
    "    tokens = data_group['Word_idx'].to_list()\n",
    "    maxlen = max([len(s) for s in tokens])\n",
    "    pad_tokens = pad_sequences(tokens, maxlen = maxlen, padding='post')\n",
    "    \n",
    "    #Pad Tags (y var) and convert it into one hot encoding\n",
    "    tags = data_group['Tag_idx'].to_list()\n",
    "    pad_tags = pad_sequences(tags, maxlen, padding='post')\n",
    "    pad_tags = [to_categorical(i, num_classes=n_tag+1) for i in pad_tags]\n",
    "    # +1 is added for 0 value\n",
    "    \n",
    "    #Split train, test and validation set\n",
    "    tokens_, test_tokens, tags_, test_tags = train_test_split(pad_tokens, pad_tags, test_size=0.1, random_state=40)\n",
    "    train_tokens, val_tokens, train_tags, val_tags = train_test_split(tokens_, tags_, test_size=0.25, random_state=40)\n",
    "    \n",
    "    print(f\"\"\"Train Tokens Length: {len(train_tokens)}\n",
    "Train Tags Lenght: {len(train_tags)}\n",
    "Test Tokens Lenght: {len(test_tokens)}\n",
    "Test Tags Lenght: {len(test_tags)}\n",
    "val Tokens Lenght: {len(val_tokens)}\n",
    "val Tags Lenght: {len(val_tags)}\"\"\")\n",
    "    \n",
    "    return train_tokens, val_tokens, test_tokens, train_tags, val_tags, test_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Tokens Length: 32372\n",
      "Train Tags Lenght: 32372\n",
      "Test Tokens Lenght: 4796\n",
      "Test Tags Lenght: 4796\n",
      "val Tokens Lenght: 10791\n",
      "val Tags Lenght: 10791\n"
     ]
    }
   ],
   "source": [
    "train_tokens, val_tokens, test_tokens, train_tags, val_tags, test_tags = get_pad_train_test_val(data_group, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential, Model, Input\n",
    "from tensorflow.keras.layers import LSTM, Embedding, Dense, \\\n",
    "TimeDistributed, Dropout, Bidirectional\n",
    "from tensorflow.keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Setting Random Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "tf.random.set_seed(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_dim:  35179 \n",
      "output_dim:  64 \n",
      "input_length:  104 \n",
      "n_tags:  18\n"
     ]
    }
   ],
   "source": [
    "input_dim = len(set(data['Word']))+1\n",
    "output_dim = 64\n",
    "input_length = max([len(s) for s in data_group['Word_idx'].tolist()])\n",
    "n_tags = len(tag2idx)+1\n",
    "print('input_dim: ', input_dim, '\\noutput_dim: ', output_dim, '\\ninput_length: ', input_length, '\\nn_tags: ', n_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bilstm_lstm_model():\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Add Embedding Layer\n",
    "    emb_matrix = Embedding(input_dim = input_dim, output_dim=output_dim, input_length=input_length)\n",
    "    \n",
    "    # Add bidirectional LSTM\n",
    "    bidir_lstm = Bidirectional(LSTM(units=output_dim, return_sequences=True, dropout=0.2 ))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bilstm_lstm_model():\n",
    "    model = Sequential()\n",
    "\n",
    "    # Add Embedding layer\n",
    "    model.add(Embedding(input_dim=input_dim, output_dim=output_dim, input_length=input_length))\n",
    "\n",
    "    # Add bidirectional LSTM\n",
    "    model.add(Bidirectional(LSTM(units=output_dim, return_sequences=True, dropout=0.2, recurrent_dropout=0.2), merge_mode = 'concat'))\n",
    "\n",
    "    # Add LSTM\n",
    "    model.add(LSTM(units=output_dim, return_sequences=True, dropout=0.5, recurrent_dropout=0.5))\n",
    "\n",
    "    # Add timeDistributed Layer\n",
    "    model.add(TimeDistributed(Dense(n_tags, activation=\"relu\")))\n",
    "\n",
    "    #Optimiser \n",
    "    # adam = k.optimizers.Adam(lr=0.0005, beta_1=0.9, beta_2=0.999)\n",
    "\n",
    "    # Compile model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(X, y, model):\n",
    "    loss = list()\n",
    "    for i in range(25):\n",
    "        # fit model for one epoch on this sequence\n",
    "        hist = model.fit(X, y, batch_size=1000, verbose=1, epochs=1, validation_split=0.2)\n",
    "        loss.append(hist.history['loss'][0])\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 104, 64)           2251456   \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 104, 128)          66048     \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 104, 64)           49408     \n",
      "_________________________________________________________________\n",
      "time_distributed_2 (TimeDist (None, 104, 18)           1170      \n",
      "=================================================================\n",
      "Total params: 2,368,082\n",
      "Trainable params: 2,368,082\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Failed to import pydot. You must install pydot and graphviz for `pydotprint` to work.\n",
      "Train on 25897 samples, validate on 6475 samples\n",
      "25897/25897 [==============================] - 109s 4ms/sample - loss: 1.1260 - accuracy: 0.8879 - val_loss: 0.4154 - val_accuracy: 0.9650\n",
      "Train on 25897 samples, validate on 6475 samples\n",
      "25897/25897 [==============================] - 95s 4ms/sample - loss: 0.4034 - accuracy: 0.9595 - val_loss: 0.3165 - val_accuracy: 0.9656\n",
      "Train on 25897 samples, validate on 6475 samples\n",
      "25897/25897 [==============================] - 95s 4ms/sample - loss: 0.3400 - accuracy: 0.9608 - val_loss: 0.2959 - val_accuracy: 0.9638\n",
      "Train on 25897 samples, validate on 6475 samples\n",
      "25897/25897 [==============================] - 108s 4ms/sample - loss: 0.3031 - accuracy: 0.9613 - val_loss: 0.2557 - val_accuracy: 0.9654\n",
      "Train on 25897 samples, validate on 6475 samples\n",
      "25897/25897 [==============================] - 117s 5ms/sample - loss: 0.2880 - accuracy: 0.9616 - val_loss: 0.2475 - val_accuracy: 0.9659\n",
      "Train on 25897 samples, validate on 6475 samples\n",
      "25897/25897 [==============================] - 94s 4ms/sample - loss: 0.2670 - accuracy: 0.9622 - val_loss: 0.2171 - val_accuracy: 0.9659\n",
      "Train on 25897 samples, validate on 6475 samples\n",
      "25897/25897 [==============================] - 94s 4ms/sample - loss: 0.2771 - accuracy: 0.9601 - val_loss: 0.2884 - val_accuracy: 0.9629\n",
      "Train on 25897 samples, validate on 6475 samples\n",
      "25897/25897 [==============================] - 90s 3ms/sample - loss: 0.3001 - accuracy: 0.9602 - val_loss: 0.2551 - val_accuracy: 0.9639\n",
      "Train on 25897 samples, validate on 6475 samples\n",
      "25897/25897 [==============================] - 99s 4ms/sample - loss: 0.2661 - accuracy: 0.9617 - val_loss: 0.2247 - val_accuracy: 0.9662\n",
      "Train on 25897 samples, validate on 6475 samples\n",
      "25897/25897 [==============================] - 97s 4ms/sample - loss: 0.2400 - accuracy: 0.9621 - val_loss: 0.2160 - val_accuracy: 0.9629\n",
      "Train on 25897 samples, validate on 6475 samples\n",
      "25897/25897 [==============================] - 96s 4ms/sample - loss: 0.2336 - accuracy: 0.9617 - val_loss: 0.2007 - val_accuracy: 0.9659\n",
      "Train on 25897 samples, validate on 6475 samples\n",
      "25897/25897 [==============================] - 95s 4ms/sample - loss: 0.2105 - accuracy: 0.9623 - val_loss: 0.1742 - val_accuracy: 0.9653\n",
      "Train on 25897 samples, validate on 6475 samples\n",
      "25897/25897 [==============================] - 102s 4ms/sample - loss: 0.1948 - accuracy: 0.9630 - val_loss: 0.1632 - val_accuracy: 0.9660\n",
      "Train on 25897 samples, validate on 6475 samples\n",
      "25897/25897 [==============================] - 95s 4ms/sample - loss: 0.1755 - accuracy: 0.9631 - val_loss: 0.1478 - val_accuracy: 0.9659\n",
      "Train on 25897 samples, validate on 6475 samples\n",
      "25897/25897 [==============================] - 95s 4ms/sample - loss: 0.1627 - accuracy: 0.9633 - val_loss: 0.1423 - val_accuracy: 0.9662\n",
      "Train on 25897 samples, validate on 6475 samples\n",
      "25897/25897 [==============================] - 96s 4ms/sample - loss: 0.1963 - accuracy: 0.9599 - val_loss: 0.1844 - val_accuracy: 0.9618\n",
      "Train on 25897 samples, validate on 6475 samples\n",
      "25897/25897 [==============================] - 101s 4ms/sample - loss: 0.1834 - accuracy: 0.9621 - val_loss: 0.1462 - val_accuracy: 0.9645\n",
      "Train on 25897 samples, validate on 6475 samples\n",
      "25897/25897 [==============================] - 94s 4ms/sample - loss: 0.1565 - accuracy: 0.9631 - val_loss: 0.1360 - val_accuracy: 0.9670\n",
      "Train on 25897 samples, validate on 6475 samples\n",
      "25897/25897 [==============================] - 100s 4ms/sample - loss: 0.1502 - accuracy: 0.9640 - val_loss: 0.1359 - val_accuracy: 0.9672\n",
      "Train on 25897 samples, validate on 6475 samples\n",
      "25897/25897 [==============================] - 95s 4ms/sample - loss: 0.1459 - accuracy: 0.9643 - val_loss: 0.1382 - val_accuracy: 0.9673\n",
      "Train on 25897 samples, validate on 6475 samples\n",
      "25897/25897 [==============================] - 103s 4ms/sample - loss: 0.1403 - accuracy: 0.9644 - val_loss: 0.1279 - val_accuracy: 0.9671\n",
      "Train on 25897 samples, validate on 6475 samples\n",
      "25897/25897 [==============================] - 99s 4ms/sample - loss: 0.1377 - accuracy: 0.9645 - val_loss: 0.1295 - val_accuracy: 0.9676\n",
      "Train on 25897 samples, validate on 6475 samples\n",
      "25897/25897 [==============================] - 94s 4ms/sample - loss: 0.1943 - accuracy: 0.9627 - val_loss: 0.2233 - val_accuracy: 0.9654\n",
      "Train on 25897 samples, validate on 6475 samples\n",
      "25897/25897 [==============================] - 90s 3ms/sample - loss: 0.2047 - accuracy: 0.9628 - val_loss: 0.1619 - val_accuracy: 0.9673\n",
      "Train on 25897 samples, validate on 6475 samples\n",
      "25897/25897 [==============================] - 90s 3ms/sample - loss: 0.1592 - accuracy: 0.9641 - val_loss: 0.1416 - val_accuracy: 0.9672\n"
     ]
    }
   ],
   "source": [
    "results = pd.DataFrame()\n",
    "model_bilstm_lstm = get_bilstm_lstm_model()\n",
    "plot_model(model_bilstm_lstm)\n",
    "results['with_add_lstm'] = train_model(train_tokens, np.array(train_tags), model_bilstm_lstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x14c549c50>]"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(results['with_add_lstm'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Future Direction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Try adding EOF in each sentence and see if affects the performance\n",
    "* Can we use POS tagging as input features along with the tokens for Named entity recognition? If so, would it improve its performance?\n",
    "* Try fused batch norm instead of batch norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
